\documentclass[a4paper, 12pt]{report}

% Packages
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[T1]{fontenc}    % Font encoding
\usepackage[english]{babel}  % Language setting
\usepackage{geometry}       % Page layout
\usepackage{setspace}       % Line spacing
\usepackage{graphicx}       % For including images
\usepackage{listings}       % For code listings
\usepackage{amsmath}        % For mathematical symbols and environments
\usepackage{hyperref}       % For hyperlinks
\usepackage{caption}        % For customized captions
\usepackage{subcaption}     % For subfigures
\usepackage{xcolor}         % For defining colors
\usepackage{tabularx}       % For defining tables
\usepackage{csquotes}       % For defining tables

% Page layout
\geometry{margin=1in} % Adjust margins as needed
\setlength{\parindent}{0pt} % No paragraph indentation
\setlength{\parskip}{1em}   % Add space between paragraphs
\onehalfspacing % Adjust line spacing as needed

% Code listing setup

% Title and Author
\title{Learning From Differences}
\author{Oliver Linger}
\date{\today}

\begin{document}

\maketitle

\vspace*{\fill} % Vertical space fill

\centering
\textbf{Final-Year Project - B.Sc. in Computer Science}

\textbf{Supervisor:} Dr. Derek Bridge

\textbf{Department of Computer Science}

\textbf{University College Cork}

\vspace*{\fill} % Vertical space fill

% Abstract
\begin{abstract}
    % Your abstract goes here
\end{abstract}


% Declaration of Originality
\section*{Declaration of Originality}
In signing this declaration, you are confirming, in writing that the submitted
work is entirely your own original work, except where it clearly attributed otherwise,
and that it has not been submitted partly or wholly for any other educational award.
\begin{enumerate}
    \item This body of work is all my own, unless clearly stated otherwise, with the full and proper accreditation;
    \item With respect to my work: none of it has been submitted to any educational institution contributing in any way to an educational award.
    \item With respect to anyone else's work: all diagrams, text, code, or ideas have been duly attributed to the source in a scholarly manner,
          whether from books, papers, lecture notes or any other student's work, whether published or unpublished, electronically or print.
\end{enumerate}

\vspace{2em} % Vertical space for the signature

\begin{flushright}
    \textit{Signed} \\
    \rule{6cm}{0.4pt} \\
    \textit{Date: \today}
\end{flushright}

\section{Acknowledgements}


% Table of Contents
\tableofcontents

% List of Figures and Tables
\listoffigures

% List of tables 
\listoftables

% Main Sections
\chapter{Introduction}
\label{ch:introduction}
% Your introduction content goes here


% Start your content here
\chapter{Literature Review}
\label{ch:Literature Review}
In recent years, the application of neural networks in regression tasks has gained significant attention within the realm of machine learning and artificial intelligence.
Applying neural networks to regression tasks has become common practice within the machine learning community \cite{neuralNetForRegression}.
Traditionally, supervised learning approaches involve presenting datasets described by features alongside their corresponding expected values to the artificial neural network (ANN),
facilitating the learning process aimed at predicting these expected values.

Inspired by case-based reasoning, which involves solving new problems by adapting solutions from old ones \cite{riesbeck2013inside},
researchers have developed predictive ensemble models based on the principles of case-based reasoning (CBR).
These principles involve retrieving the most similar cases, reusing them to attempt to solve the problem, revising the proposed solution if necessary,
and retaining the new solution as part of a new case \cite{watsonCaseBasedReasoningReview}.

Researchers have suggested the potential advantages of deviating from this conventional paradigm by training neural networks on differences between sets of features to predict
differences between values. This departure from the traditional approach underscores a shift towards a more nuanced understanding of the learning process,
emphasizing the exploration of alternative methodologies to enhance the efficiency and efficacy of neural network training.
Additionally, it aims to provide more transparency within the learning process to offer insights into the black box that is a neural network.

\section{KNN + ANN for Regression, utilizing (CBR)}

The exploration of alternative methodologies for enhancing the performance of neural networks in regression tasks has attracted considerable attention within the machine
learning community in recent years. One innovative approach, inspired by the principles of case-based reasoning (CBR),
involves training neural networks to predict differences between values based on differences between sets of features.

This departure from the conventional supervised learning paradigm aims to leverage the inherent advantages of learning from differences, potentially
leading to more interpretable and efficient models. The study under consideration \cite{learningFromDifferences2022} conducts a
systematic factorial study to investigate the efficacy of this approach across various datasets and experimental conditions.
The findings suggest that neural networks trained on differences demonstrate comparable or even superior performance to those trained using traditional methods,
while requiring significantly fewer epochs for convergence. In this section of the literature review, we consider the utilization of case-based reasoning for regression tasks,
with a specific focus on the implementation of KNN (K-Nearest Neighbors) and ANN (Artificial Neural Network) frameworks.

\subsection{Learning From Differences Paper}

The paper by \cite{learningFromDifferences2022} introduces the innovative concept of training a neural network based on the disparities between a base case and its closest neighbors.
Titled "A Factorial Study of Neural Network Learning from Differences for Regression" \cite{learningFromDifferences2022},
the study aims to present novel learning methodologies geared towards enhancing performance and fostering a more interpretable learning process.
The research evaluates three model variations: a benchmark neural network, a learning from differences model, and an augmented learning from differences model incorporating the original context.
The context here refers to the base case, from which differences with its nearest neighbors are calculated.

According to \cite{learningFromDifferences2022}, the study's key findings underscore a significant increase in accuracy when incorporating contextual information in the learning process.
Moreover, comparable or superior results were achieved with fewer training epochs compared to the basic neural network.
This enhancement is attributed partly to the broader training dataset that encompasses neighbors greater than 1.
Notably, the paper advocates for a hybrid approach, combining learning from differences with direct feature-based learning, which yields superior results.

The related work for the paper \cite{learningFromDifferences2022} has addressed the use of neural networks in the case-based reasoning process.
These papers have focused on exploiting the base case using the idea that adaptation of information is acquirable from the differences between pairs \cite{hanney1996learning}.
Since then, various different approaches have been used. More recently, in relation to using neural networks to predict the differences between problems,
a few interesting points have been investigated. These points show the ability of a neural network to correct the solution of the most similar retrieved case.
These preliminary works expressed in the paper showed that the use of differences plus context yielded superior results \cite{craw2006learning}.
The previous studies examined in the paper \cite{learningFromDifferences2022} did not examine the impact of different parameters during the (CBR) case-based reasoning experiments.
The paper \cite{learningFromDifferences2022} made an effort to explore the effect of epochs in the learning process.

The graphical representations in the paper highlight a distinct trend: rapid attainment of high accuracy levels with the learning from differences method,
particularly when contextual information is included. Conversely, the basic neural network requires a substantially larger number of epochs to achieve comparable accuracy,
if not worse. This disparity suggests the efficiency of the learning from differences approach.

In conclusion, the paper suggests a marginal performance enhancement and a substantial reduction in training epochs through the adoption of the differences method
and inclusion of contextual information. However, it is crucial to acknowledge the time overhead associated with retrieving and training using similar cases.

The process of learning from differences for regression, as described in the paper by \cite{learningFromDifferences2022},
demonstrates the use of the case difference heuristic in the creation of a predictive model.
Consider a dataset $D$ of pairs $(X_i, Y_i)$, where $Y_i$ is a vector of $n$ numerical values corresponding to the result we expect to predict. Within the paper,
the method of executing a regression model follows the normal pattern of training and predicting that a standard model would. However,
instead of the features themselves being passed into the neural net,
the differences between the features and their nearest neighbors are passed in. The differences set is represented by $\Delta D^{\text{train}}$.
The network, after being fitted with the above, is used again to make predictions. This time, the test set is used,
where nearest neighbors in the training set are retrieved using the items in the training set $D^{\text{train}}$.
The resultant dataset of test differences is called $\Delta D^{\text{test}}$. Predictions are made on this dataset,
and the predictions are added to their corresponding differences in $D^{\text{train}}$.
The average of the resultant label is taken as the regression prediction.

The paper \cite{learningFromDifferences2022} has an associated library where they have implemented a regression-based machine learning model.

The new model proposed in the paper is tested using the factorial study methodology.
This process involved finding a network structure through experimental means for each dataset.
It entailed training and testing using a wide range of hyperparameters to determine the optimal settings.

Despite its contributions, the paper falls short in adequately addressing classification and exploring additional variations of the learning from differences model beyond context inclusion.
Incorporating original values for the base case in the generated differences dataset provides a valuable framework for understanding the observed disparities.
While the model with contextual inclusion demonstrates slight improvements over the base model in learning from differences, a more comprehensive examination of the methodology
is warranted for further refinement and enhancement.

The paper tested three models using the same factorial methodology: a base version, which is a basic neural network trained empirically;
a differences model employing the (CBH) Case-based Heuristic for predictions; and a differences model with its base context included. The results yielded interesting insights,
with similar performance for all three models and a slight improvement in the differences $+$ context model.
Notably, the differences and differences $+$ context models required significantly fewer epochs to achieve accuracy compared to the basic neural network,
which achieved similar accuracy after more epochs.

Another consideration in the paper \cite{learningFromDifferences2022} was the number of neighbors for both training and prediction.
The study revealed no consistent trend across datasets, suggesting dataset-specific requirements.
This approach could be seen as a method of data augmentation, as choosing a larger number of neighbors dramatically increases the dataset size.

In conclusion, the learning from differences method achieved equal or higher accuracy compared to the base neural network model with far fewer epochs,
potentially leading to faster training times. However, careful consideration must be given to the number of neighbors used, as increasing it can lead to significantly higher time overhead.

An improvement on the study \cite{learningFromDifferences2022} would be to vary the network structure, including but not limited to increasing the number of layers and neurons.
Additionally, including datasets that require different types of networks could provide valuable insights.
Exploring these parameters could lead to improvements in the existing learning from differences regression model.
While the study tested a singular variant through the inclusion of context, exploring more variants of the model could yield further enhancements.
% End of subsection
\section{Learning Adaptations for Case-based classification}
The paper \cite{ye2021learning} "Learning Adaptations for Case-based Classification" explores the implementation of Case-Based Reasoning (CBR) for classification.
It investigates recent advancements in CBR and its replacement of learning rules with Case-Based Heuristic (CBH) network models for adaptation.
The paper introduces a novel model with three variations: segmentation of adaptation knowledge based on the classes of the source cases,
training a single neural network on differences between problem solutions, and adapting from an ensemble of source cases followed by a majority vote.

This study \cite{ye2021learning} presents the first neural network-based approach to classification, called "C-NN-CDH" or
Classification with Neural Network-based Case Difference Heuristic. The approach uses a neural network to learn adaptation knowledge from pairs of cases.
According to the paper, this approach outperforms traditional statistical models.

The first approach involves segmenting the dataset by class and training a neural network for all segmented differences set between base cases.
This method offers faster training but slightly less accuracy, according to the paper. The use of these CBR methods provides at least two benefits:
inertia-free lazy learning and the ability to assess different cases when running classification. Inertial free means that the model does not make
assumptions based on the underlying structure of the data, and lazy learning pertains to an algorithm that will postpone generalization until it
is certain a new instance needs to be classified. It will only classify as the data comes; it won't try and learn or adapt preemptively.

In the process of CBR, adaptation is often treated as the most difficult. Various methods have been developed by academics over the years to tackle the issue.
For example, \cite{leake1996acquiring} extrapolates a method in which the case base adaptation cases are populated from previously successful adaptations.
Another approach is to have a stored base case and query and retrieve the case most similar to it,
and adapting the retrieved case to the base case \cite{craw2006learning}.

In the paper \cite{ye2021learning}, several design questions had to be addressed.
How to calculate problem differences and how to select case pairs for training. The paper in question \cite{ye2021learning} considers standard
pair selection methods as well as a new training per selection approach based on class-class classification.

The paper \cite{ye2021learning} addresses the application of (CDH) method for classification.
It discusses the value distance metric which is  probabilistic method
to measure the similarity that allows the comparison of nominal values in a single dimensional space.

\subsection{An NN-CDH Approach for Classification}
The NN-CDH method is a novel approach to classification that operates through pairs of cases, where one serves as the source and the other as the target.
This method revolves around adapting the source case to the target case, forming what is commonly known as a case pair.

To achieve adaptation, a neural network is integrated into the process. The difference between the source and target cases is computed, and the neural network is trained on these differences.

The neural network computes the difference between the source and target pairs.
This difference calculation is a crucial step in the Case-Based Reasoning (CBR) system.

Once the difference is computed, the predicted result obtained from the neural network is applied to the source solution.
This adapted solution is then considered as the final result of the classification process.

The NN-CDH approach represents a fusion of neural network techniques with the principles of Case-Based Reasoning, offering a dynamic and adaptable method for classification tasks.

When calculating the difference between the problem and the solution values it is important to implement a difference function.
This presents an interesting challenge for nominal values. The method presented in \cite{ye2021learning} uses an implicit calculation through machine learning techniques.
This replaces tradition (CDH) case difference heuristic approaches for difference calculations. Through the use of a neural network the papers' method hopes to include the base context in the
differences' calculation. This method for generating differences has been aptly named ("C-CDH") case difference heuristic approach for classification.

Multiple variants were created with the paper \cite{ye2021learning}.
\subsection{Variant 0: Non-Network C-CDH}

Variant 0, as described in \cite{ye2021learning}, serves as a foundational test bed implementation for classification tasks.

\begin{itemize}
    \item \textbf{Storage of Case Pairs:} The C-CDH system stores case pairs, treating them as adaptation rules. Each pair consists of a source case and a target case.

    \item \textbf{Grouping Based on Source Solution:} Case pairs are grouped based on the source solution, facilitating efficient retrieval and comparison during classification.

    \item \textbf{Classification Process:}
          \begin{enumerate}
              \item The system retrieves the most similar source case from the stored case pairs.
              \item It selects the case pair with the most similar source problem and target problem among those sharing the same source solution.
              \item The target solution of the retrieved case pair is utilized for the final classification.
          \end{enumerate}

    \item \textbf{Final Outcome:} The final classification outcome is determined based on the target solution of the retrieved case pair.
\end{itemize}

\subsection{Variant 1: C-NN-CDH}

Variant 1, referred to as C-NN-CDH from the paper \cite{ye2021learning}, extends the foundational principles established in Variant 0, 
aiming to integrate neural networks into the classification process while maintaining the core functionalities of the C-CDH system. Below is the description of Variant 1:

\begin{itemize}
    \item \textbf{Storage of Case Pairs:} Similar to Variant 0, the C-NN-CDH system stores case pairs as adaptation rules. 
    Each case pair comprises a source case, a target case, and additional information related to probabilities.
    These probabilities could be obtained from the output of the neural network or calculated based on certain features of the data.
    
    \item \textbf{Grouping Based on Source Solution:} Case pairs are organized based on the source solution, 
    ensuring efficient retrieval and comparison during the classification process.
    
    \item \textbf{Classification Process:} 
    \begin{enumerate}
        \item The system retrieves the most similar source case from the stored case pairs. This can be done with euclidean distance via Nearest Neighbors.
        \item In variant 1, probabilities of the source, target, and solution of the target are included.
        These probabilities could be generated by the neural network model based on the input data.
        \item For Variant 1:
            \begin{itemize}
                \item The system appends pairs of probabilities from the source and target classes.
                \item It selects the case pair with the most similar source problem and target problem among those sharing the same source solution.
            \end{itemize}
        \item The target solution of the retrieved case pair is utilized for the final classification.
    \end{enumerate}
    
    \item \textbf{Final Outcome:} Similar to Variant 0, the final classification outcome is determined based on the target solution of the retrieved case pair.
\end{itemize}

\subsection{Variant 2: C-NN-CDH}

Variant 2, is very similar to variant 1 but includes the source solution during the training of its neural network.

\begin{itemize}
    \item \textbf{Storage of Case Pairs:} Similar to Variant 0, and 1, the variant 2 system stores case pairs as adaptation rules. 
    Each case pair comprises a source case, a target case, and additional information related to probabilities.
    These probabilities could be obtained from the output of the neural network or calculated based on certain features of the data.
    
    \item \textbf{Grouping Based on Source Solution:} Case pairs are organized based on the source solution, 
    ensuring efficient retrieval and comparison during the classification process.
    
    \item \textbf{Classification Process:} 
    \begin{enumerate}
        \item The system retrieves the most similar source case from the stored case pairs. This can be done with euclidean distance via Nearest Neighbors.
        \item In variant 2 probabilities of the source, target, solution of the target, and the source solution are included.
        These probabilities could be generated by the neural network model based on the input data.
        \item For Variant 1:
            \begin{itemize}
                \item The system appends pairs of probabilities from the source and target classes.
                \item It selects the case pair with the most similar source problem and target problem among those sharing the same source solution.
            \end{itemize}
        \item The target solution of the retrieved case pair is utilized for the final classification.
    \end{enumerate}
    
    \item \textbf{Final Outcome:} Similar to Variant 0, and 1, the final classification outcome is determined based on the target solution of the retrieved case pair.
\end{itemize}

\subsection{Variant 3 - 5: C-NN-CDH}
The variants 3 - 5 for classification stored in the paper \cite{ye2021learning} add the grouping of case pairs based on their source solutions. Giving multiple specialized adaptation neural networks,
that have learned to adapt cases of specific solutions to all solutions. The variants 3 - 5 all share the same grouping and training method but vary in their testing methods.
It has been inspired by the ensemble of adaptations for classification approach (EAC) \cite{jalali2017learning}.

\subsubsection{Variant 3: Single Adaptation Neural Network}

Variant 3 introduces a specialized adaptation neural network approach,
which predicts the target solution by retrieving the most similar source case using 1-Nearest neighbor and employing one specialized adaptation neural network for adaptation.


\textbf{Characteristics:}

\begin{itemize}
    \item \textbf{Grouping of Case Pairs:} Case pairs are grouped based on their source solutions. 
    The target solutions within a group are not restricted. 
    Meaning there is no constraint on the possible solutions. 
    
    \item \textbf{Training Procedure:} Each specialized adaptation neural network is trained on a specific group of case pairs, those case pairs with the same class outcome, 
    focusing on learning adaptation knowledge where the source solution is determined. 
    The separation of case pairs based on source solutions enables the incorporation of the source solution as a crucial input because it is already known, guiding the selection of the appropriate specialized adaptation neural network.
    
    \item \textbf{Homogeneous Training:} Training becomes more manageable as each group of case pairs is more homogeneous, 
    and the knowledge to be learned is more specific. This specialization improves the adaptation process.
\end{itemize}

\textbf{Testing Procedure:}

\begin{itemize}
    \item During the testing phase, the system retrieves the most similar source case for each test case using its nearest neighbor (1-NN).
    \item It utilizes one specialized adaptation neural network corresponding to the retrieved source case.
    \item The target solution is predicted based on the alteration performed by the selected specialized neural network.
\end{itemize}

\subsubsection{Variant 4: N Adaptation Neural Network}

Variant 4 also has a specialized adaptation neural network approach,
which predicts the target solution by retrieving the most similar source case using K-Nearest neighbor and employing one specialized adaptation neural network for adaptation.
It's use of (k-NN) gives it an advantage in the number of predictions it can make to then take a vote on the most likely solution.

\textbf{Characteristics:}

\begin{itemize}
    \item \textbf{Grouping of Case Pairs:} Is the same as variant 3. 
    
    \item \textbf{Training Procedure:} Is the same as variant 3. 
    
    \item \textbf{Homogeneous Training:} Is the same as variant 3. 
\end{itemize}

\textbf{Testing Procedure:}

\begin{itemize}
    \item During the testing phase, the system retrieves the most similar source cases for each test case using (K-NN) K-nearest neighbors (K-NN).
    \item It utilizes one specialized adaptation neural network corresponding to the retrieved source case.
    \item The target solutions are created using the adaptation created by the specialized neural network, applied to the retrieved cases.
    \item A majority vote is taken and returned as the final outcome.
\end{itemize}

\subsubsection{Variant 5: N Adaptation Neural Network, taking N neighbors from all classes}

    Variant 5 is based off variant 4, its main difference being that it uses N neighbors from all classes. Giving it the use of every specialized neural network.
    
    \textbf{Characteristics:}
    
    \begin{itemize}
        \item \textbf{Grouping of Case Pairs:} Is the same as variant 3. 
        
        \item \textbf{Training Procedure:} Is the same as variant 3. 
        
        \item \textbf{Homogeneous Training:} Is the same as variant 3. 
    \end{itemize}
    
    \textbf{Testing Procedure:}
    
    \begin{itemize}
        \item During the testing phase, the system retrieves the most similar source cases for each test case using (K-NN) K-nearest neighbors (K-NN), for every class. It retrieves k neighbors for each distinctive class.
        \item It utilizes one specialized adaptation neural network corresponding to the retrieved source case.
        \item The target solutions are created using the adaptation created by the specialized neural network, applied to the retrieved cases.
        \item A majority vote is taken and returned as the final outcome.
\end{itemize}

\subsection{Evaluation of Learning adaptations for case based classification: A Neural Network approach}
\subsubsection{Gathering Case Based Pairs}
\begin{itemize}
    \item \textbf{Neighboring pairs:} Each source case will be paired with its nearest neighbor.
    \item \textbf{Random pairs:} Each source case is paired with 10 random cases. This results in 10n random pairs.
    \item \textbf{Class-to-Class pairs:} each source case is paired with its  nearest neighbor in every other class resulting in n(m - 1) pairs, we exclude that source cases class.
\end{itemize}

\subsubsection{Evaluation Summary}

Within the paper \cite{ye2021learning} the experiments conducted on two different datasets provided answers to several key questions.

The effectiveness of neural network learning adaptation knowledge was demonstrated through consistent performance of one or more C-NN-CDHs, 
which often outperformed the neural network classifier. Despite the possibility of neural networks learning to discard the source problem and rely solely on the target problem, 
the non-zero weights associated with the source problem and significant performance differences between C-NN-CDHs and the neural network indicate that C-NN-CDHs effectively learn adaptation knowledge. 
This suggests that if a neural network can handle the classification task directly, it can also learn adaptation knowledge or the relation between pairs of cases in the task domain.

Surprisingly, variant (1) performed almost identically to variant (2), 
which also considers the source solution in adaptation. 
This suggests that the source solution, heavily coupled with the source problem, may not provide additional useful information for adaptation.

In terms of segmenting case pairs by source case solution, variants (1, 2) generally outperformed variants (3, 4) in accuracy on most datasets. 
This could be because a single adaptation neural network in (1, 2) is well-trained with all pairs of cases, while a specialized adaptation neural network in (3, 4) is trained with segmented examples. 
However, variants (3, 4) showed faster convergence during training due to training on segmented examples.

The impact of the ensemble of adaptations was investigated, with EAC-NN-CDH (variant (4)) performing similarly to its counterpart variant (3) without ensemble. 
This suggests that the generalization power of C-NN-CDH produces stable predictions that an ensemble version does not significantly alter.

The usefulness of the class-to-class approach for adaptation was demonstrated by C2C-NN-CDH (variant (5)), which performed differently from and often better than the other C-NN-CDHs. 
C2C-NN-CDH reaches its prediction by collecting evidence from diverse source cases from all classes, 
which can provide more global support, especially when there are multiple classes. Moreover, 
the C2C approach offers the possibility of explanation with contrastive evidence.

\subsubsection{Conclusions on Learning adaptations for case based classification: A Neural Network approach}
The conclusion of the paper \cite{ye2021learning} being that (variant (5)) is worth investigating using its C2C-NN-CDH approach to classification. 
As it yielded the most promising results when compared to all the other variants tested in the paper.


\chapter{Design}
\label{ch:Design}

\section{Linger Regression Model}

\subsection{Objective}
The Linger Regression model integrates principles of Case-Based Reasoning (CBR) into regression analysis, aiming to enhance interpretability and performance. The objectives are:
\begin{itemize}
    \item Develop a dynamic regression model using differences to adapt to varying contexts.
    \item Ensure compatibility with Scikit-learn for accessibility and ease of implementation.
    \item Enable effective interpretation of predictions for users.
    \item Implement the learning from differences algorithm with enhancements for improved performance.
\end{itemize}

\subsection{Architecture Overview}
The \texttt{LingerRegressor} combines K-nearest neighbors (KNN) and \texttt{MLPRegressor} from Scikit-learn for flexibility and accuracy.

\subsection{Components}
\begin{itemize}
    \item KNN module: Computes nearest neighbors and extracts differences during both training and prediction phases.
    \item \texttt{MLPRegressor}: Offers flexibility in activation functions, solvers, and hyperparameters for regression.
    \item Supplementary Features: Weighted KNN, Context Addition, Additional Results Column, and Duplication Based on Distance.
\end{itemize}

\subsection{Parameterization}
Extensive parameterization allows fine-tuning of KNN, \texttt{MLPRegressor}, and additional features.

\subsection{Training and Prediction}
The model computes differences during training, fits the \texttt{MLPRegressor}, and identifies nearest neighbors for predictions.

\subsection{Data Collection and Preprocessing}
Details on how training and validation datasets were collected and preprocessed.

\subsection{Model Evaluation}
\begin{itemize}
    \item Model Selection: Rationale behind choosing the Linger Regression model.
    \item Evaluation Metrics: Metrics used to assess model performance.
    \item Cross-Validation and Testing: Methodology used for validation and testing.
\end{itemize}

\subsection{Infrastructure}
Hardware and software specifications for development and deployment.

\subsection{Ethical Considerations}
Discussion on ethical considerations associated with the Linger Regression model.

\subsection{Limitations and Future Work}
Acknowledgement of limitations and proposals for future improvements.


\chapter{Implementation}
\label{ch:Implementation}

\chapter{Evaluation}
\label{ch:Evaluation}

\chapter{Conclusions}
\label{ch:Conclusions}

\chapter{Experiments}
\label{ch:Experimnents}

\section{Experiment 1: Abalone dataset Linger Regression}

\section{Experiment 2: House Prices Dataset Regression}

\section{Experiment 3: Glass Dataset Classification}

\section{Experiment 4: Raisin Dataset Classification}

\section{Experiment 5: Car Safety Dataset Classification}

\chapter{Results}
\label{ch:Results}
\bibliographystyle{plain}
\bibliography{references}

\end{document}
